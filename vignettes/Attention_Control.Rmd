---
title: "Attention Control Tasks"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

There are two sets of functions for the attention control tasks:

-   `raw_`

-   `score_`

*Only the visual arrays task has a `score_` function because the scoring for the other tasks are simply a summary statistic (mean) on performance (accuracy)*

The `raw_` functions will create a tidy raw data file for the task with relevant columns for you to calculate task scores and other performance measures on the task.

The `score_` functions will create a scored data file for the task from the output provided by the `raw_` functions.

## `raw_` functions

-   `raw_antisaccade()`

-   `raw_sact()`

-   `raw_flankerDL()`

-   `raw_stroopDL()`

-   `raw_visualarrays()`

```{r eval=FALSE}
data_raw <- raw_antisaccade(data_import)
```

Trial level variables are provided by the `raw_` functions.

## `score_` functions

-   `score_visualarrays()`

Based on the output provided by the `raw_` functions, the `score_` functions will calculate task level scores. In order to allow for more flexibility with using the `score_` functions, they will need to be combined with `dplyr::group_by()`.

```{r eval=FALSE}
library(englelab)
library(dplyr)
library(tidyr)

data_raw <- raw_visualarrays(data_import)

data_scores <- data_raw %>%
  group_by(Subject) %>%
  score_visualarrays() %>%
  pivot_wider(id_cols = "Subject",
              names_from = "SetSize",
              names_prefix = "VAorient_S.k_",
              values_from = "k") %>%
  mutate(VAorient_S_k = (VAorient_S.k_5 + VAorient_S.k_7) / 2)
```

## Calculate Reliability

Any time you use a measure of individual differences it is critical to provide reliability estimates from your sample. Here is a demonstration of how to do so with the visual arrays task.

Reliability estimates should be calculated based on the trial level data that was used to calculated an aggregated score for the task. In the visual arrays task, *k* scores are calculated for each set-size separately and then aggregated. One way to calculate reliability for this task would be to split trials up into even and odd, for each set-size separately. Then calculate *k* scores for even and odd trials, for each set-size, and aggregate even trial *k* scores over the two set-sizes and do the same for odd trial *k* scores. Then calculate spearman-brown corrected split-half reliability.

### Split-Half

```{r eval=FALSE}
library(englelab)
library(dplyr)
library(tidyr)

splithalf <- data_raw %>%
  group_by(Subject, SetSize) %>%
  mutate(Split = ifelse(Trial %% 2, "odd", "even")) %>%
  group_by(Subject, SetSize, Split) %>%
  score_visualarrays() %>%
  pivot_wider(id_cols = "Subject",
              names_from = c("SetSize", "Split"),
              names_prefix = "k_",
              values_from = "k") %>%
  mutate(VAorient_S.k_even = (k_5_even + k_7_even) / 2,
         VAorient_S.k_odd = (k_5_odd + k_7_odd) / 2) %>%
  summarise(r_VA.k = cor(VAorient_S.k_even, VAorient_S.k_odd)) %>%
  mutate(r_VA.k = (2 * r_VA.k) / (1 + r_VA.k))
```
